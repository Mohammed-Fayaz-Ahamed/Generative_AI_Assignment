{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jjo1mtH4Qyo7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtw4EljURVxC",
        "outputId": "8133972f-6941-4f8a-b344-b532d2e08d2c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = \"The quick brown fox jumps over the lazy dog. The dog barks at the cat.\""
      ],
      "metadata": {
        "id": "FsB1CmrKRaSZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = word_tokenize(text_data)\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "print(\"Stopwords removed:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqPBFZajRiyB",
        "outputId": "f87d64d5-ee32-406f-f4c6-d86354bb076b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords removed: ['The', 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', '.', 'The', 'dog', 'barks', 'cat', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "sentences = sent_tokenize(text_data)\n",
        "print(\"Tokenized sentences:\", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyfEIJMmRmTh",
        "outputId": "110aacc2-652e-4fc2-ce6c-61eb2bb15315"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentences: ['The quick brown fox jumps over the lazy dog.', 'The dog barks at the cat.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVCkN5PcRqT6",
        "outputId": "61397ba6-c29d-4418-80a9-32fdf3f5b6b1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['the', 'quick', 'brown', 'fox', 'jump', 'lazi', 'dog', '.', 'the', 'dog', 'bark', 'cat', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCnlCu83RtRi",
        "outputId": "df6fe8a9-c2c0-498d-aa48-00285995428c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words: ['The', 'quick', 'brown', 'fox', 'jump', 'lazy', 'dog', '.', 'The', 'dog', 'bark', 'cat', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tagging\n",
        "pos_tags = pos_tag(filtered_words)\n",
        "print(\"POS tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PwFgiv1Rv9S",
        "outputId": "5480d0ac-6904-46b1-be7e-00d7f039d749"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('The', 'DT'), ('dog', 'NN'), ('barks', 'VBZ'), ('cat', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(sentences)\n",
        "print(\"TF-IDF vectors:\", tfidf_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSbPVDyNRzFB",
        "outputId": "d06569cc-88f9-4e34-9f30-b1498fe14edf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF vectors:   (0, 4)\t0.24359836256665346\n",
            "  (0, 7)\t0.34236899897971346\n",
            "  (0, 8)\t0.34236899897971346\n",
            "  (0, 6)\t0.34236899897971346\n",
            "  (0, 5)\t0.34236899897971346\n",
            "  (0, 2)\t0.34236899897971346\n",
            "  (0, 9)\t0.34236899897971346\n",
            "  (0, 10)\t0.4871967251333069\n",
            "  (1, 3)\t0.4251963615908802\n",
            "  (1, 0)\t0.4251963615908802\n",
            "  (1, 1)\t0.4251963615908802\n",
            "  (1, 4)\t0.3025307132406998\n",
            "  (1, 10)\t0.6050614264813996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "one_hot_vectors = vectorizer.fit_transform(filtered_words)\n",
        "print(\"One-hot vectors:\", one_hot_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwX_FL4ER12h",
        "outputId": "b525445c-ac86-4142-9d51-6bb51ad494e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot vectors:   (0, 8)\t1\n",
            "  (1, 7)\t1\n",
            "  (2, 1)\t1\n",
            "  (3, 4)\t1\n",
            "  (4, 5)\t1\n",
            "  (5, 6)\t1\n",
            "  (6, 3)\t1\n",
            "  (8, 8)\t1\n",
            "  (9, 3)\t1\n",
            "  (10, 0)\t1\n",
            "  (11, 2)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_vectors = bow_vectorizer.fit_transform(sentences)\n",
        "print(\"Bag of Words vectors:\", bow_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw_yAvE9R4iJ",
        "outputId": "6ee6e524-1730-4383-9d3c-ca14da3e19dd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words vectors:   (0, 10)\t2\n",
            "  (0, 9)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 4)\t1\n",
            "  (1, 10)\t2\n",
            "  (1, 4)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 3)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unigram, Bigram, n-gram\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3))  # Unigrams, bigrams, and trigrams\n",
        "ngram_vectors = ngram_vectorizer.fit_transform(sentences)\n",
        "print(\"n-gram vectors:\", ngram_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSC2XvHiR7Pp",
        "outputId": "97772efc-ecc3-40cf-96ea-e23c9c52d3a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n-gram vectors:   (0, 27)\t2\n",
            "  (0, 24)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 16)\t1\n",
            "  (0, 21)\t1\n",
            "  (0, 19)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 33)\t1\n",
            "  (0, 25)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 14)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 22)\t1\n",
            "  (0, 31)\t1\n",
            "  (0, 20)\t1\n",
            "  (0, 34)\t1\n",
            "  (0, 26)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 18)\t1\n",
            "  (0, 23)\t1\n",
            "  (0, 32)\t1\n",
            "  (1, 27)\t2\n",
            "  (1, 10)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 9)\t1\n",
            "  (1, 29)\t1\n",
            "  (1, 11)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 28)\t1\n",
            "  (1, 30)\t1\n",
            "  (1, 12)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 2)\t1\n"
          ]
        }
      ]
    }
  ]
}